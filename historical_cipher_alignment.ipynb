{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cipher Alignment and Key Recreation Project\n",
    "\n",
    "This notebook demonstrates the process of aligning historical ciphertexts with their plaintexts and evaluating the recreation of cipher keys using statistical models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports dependencies for the project\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "from cipher import Cipher, CipherKey, ErrorSequence\n",
    "from cipher_alignment_model import CipherAlignmentModel, NLTKIBMModelAdapter, GenericIBMModelAdapter\n",
    "from alignment import Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATACLASSES\n",
    "@dataclass\n",
    "class KeyRecreationResult:\n",
    "    \"\"\"Dataclass for storing the results of the evaluation of a recreated cipherkey.\"\"\"\n",
    "    cipher: Cipher\n",
    "    evaluations: list[tuple[CipherAlignmentModel, tuple[float, float, float, CipherKey]]] # List of tuples with the model and the evaluation results\n",
    "    def __str__(self):\n",
    "        return f\"{self.cipher}\\n{self.evaluations}\"\n",
    "    \n",
    "@dataclass\n",
    "class AlignmentResult:\n",
    "    \"\"\"Dataclass for storing the results of the evaluations of alignments.\"\"\"\n",
    "    cipher: Cipher \n",
    "    evaluations: list[dict]  # Use a list of dictionaries to hold detailed results per alignment\n",
    "\n",
    "    def __str__(self):\n",
    "        results_str = '\\n'.join(str(evaluation) for evaluation in self.evaluations)\n",
    "        return f\"{self.cipher}\\n{results_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "def sample_lines_from_large_file(file_path, sample_size):\n",
    "    \"\"\"Sample random lines from a large file using reservoir sampling.\"\"\"\n",
    "    sampled_lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i < sample_size:\n",
    "                sampled_lines.append(line)\n",
    "            else:\n",
    "                j = random.randint(0, i)\n",
    "                if j < sample_size:\n",
    "                    sampled_lines[j] = line\n",
    "    return sampled_lines\n",
    "\n",
    "def create_row(cipher, result, model_name, n_iter)-> dict[str, Union[str, float, int, None]]:\n",
    "    \"\"\"Creates a 'row' to be used in a data-table with the information for one cipher.\"\"\"\n",
    "\n",
    "    precision = result[0]\n",
    "    recall = result[1]\n",
    "    f1_score = result[2]\n",
    "    recreated_key = result[3]\n",
    "\n",
    "    return {\n",
    "        #'homophonicity': cipher.homophonicity,\n",
    "        'csv_filename': cipher.csv_filename,\n",
    "        'homophonicity': cipher.homophonicity,\n",
    "        'error': str(cipher.error.error_type if cipher.error is not None else None),\n",
    "        'error rate': str(cipher.error.error_rate if cipher.error is not None else None),\n",
    "        'precision': round(precision, 4) if precision is not None else None,\n",
    "        'recall': round(recall, 4) if recall is not None else None,\n",
    "        'f1_score': round(f1_score, 4)if f1_score is not None else None,\n",
    "        'filename': cipher.filename,\n",
    "        'model': model_name,\n",
    "        'segment_size': cipher.seq_size,\n",
    "        'iterations': n_iter,\n",
    "        'id_tag': cipher.id_tag,\n",
    "        'missing_keys': recreated_key.check_missing_keys(cipher.original_reference_key) if recreated_key is not None else None,\n",
    "        'recreated_key': recreated_key if recreated_key is not None else None\n",
    "    }\n",
    "def write_to_file(ciphers_and_results: list[KeyRecreationResult], filename: str) -> None:\n",
    "    \"\"\"Write cipher information and evaluation results to a CSV file.\"\"\"\n",
    "    data = []\n",
    "    for result in ciphers_and_results:\n",
    "        for model, evaluation in result.evaluations:\n",
    "            data.append(create_row(result.cipher, evaluation, model.NAME, model.n_iter))\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    csv_filename = filename.replace('.xlsx', '.csv')\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Results for Cipher key written to {csv_filename}\")\n",
    "\n",
    "\n",
    "def write_alignment_to_file(ciphers_and_results: list[AlignmentResult], filename: str) -> None:\n",
    "    \"\"\"Write alignment evaluation results to a CSV file.\"\"\"\n",
    "    data = []\n",
    "    for alignment_result in ciphers_and_results:\n",
    "        for evaluation in alignment_result.evaluations:\n",
    "            data.append({\n",
    "                'cipher_id': alignment_result.cipher.id_tag,\n",
    "                'csv_filename': alignment_result.cipher.csv_filename,\n",
    "                'homophonicity': alignment_result.cipher.homophonicity,\n",
    "                'model': evaluation['model'],\n",
    "                'error': evaluation['error'],\n",
    "                'error rate': evaluation['error rate'],\n",
    "                'length': evaluation['length'],\n",
    "                'checklength': evaluation['checklength'],\n",
    "                'iterations': evaluation['iterations'],\n",
    "                'segment_size': alignment_result.cipher.seq_size,\n",
    "                'precision': evaluation['precision'],\n",
    "                'recall': evaluation['recall'],\n",
    "                'f1_score': evaluation['f1_score'],\n",
    "                'alignment': evaluation['alignments']\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    csv_filename = filename.replace('.xlsx', '.csv')\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Alignment results written to {csv_filename}\")\n",
    "\n",
    "def generate_xlsx_file_name(name:str) -> str:\n",
    "    \"\"\"Generates a file name based on the current date and time.\"\"\"\n",
    "    now = datetime.now()\n",
    "    now_str = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    return f'{name}_{now_str}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation Functions\n",
    "def train_and_evaluate_model(model: CipherAlignmentModel, cipher: Cipher) -> tuple[float, float, float, CipherKey]:\n",
    "    \"\"\"Trains the model and evaluates it on the given cipher. Returns the precision, recall and F1-score.\n",
    "    Also returns the recreated key.\"\"\"\n",
    "    reference_key=cipher.original_reference_key\n",
    "    recreated_key=CipherKey(model.translation_probs)\n",
    "    precision, recall, f1_score =recreated_key.compare_keys(reference_key)\n",
    "    return precision, recall, f1_score, recreated_key\n",
    "\n",
    "def evaluate_alignments(model: CipherAlignmentModel, cipher: Cipher, continuous=False) -> dict:\n",
    "    \"\"\"Evaluates the alignments produced by the model for the given cipher.\"\"\"\n",
    "    #print(\"EVALUATING ALIGNMENTS!!!\")\n",
    "    alignments = model.align_sentences(continuous=continuous)\n",
    "    \n",
    "    concatenated_alignment_data = []\n",
    "    for alignment_obj in alignments:\n",
    "        \n",
    "        concatenated_alignment_data.extend(alignment_obj.alignment)\n",
    "    concatenated_alignment = Alignment(concatenated_alignment_data, model.NAME)\n",
    "\n",
    "\n",
    "    print(f\"LEN: {len(concatenated_alignment)}\")\n",
    "    # Calculate metrics on the concatenated alignment\n",
    "    aer = concatenated_alignment.calculate_aer(cipher.reference_alignment_data)\n",
    "    ld = concatenated_alignment.calculate_levenshtein(cipher.reference_alignment_data)\n",
    "    precision, recall, fscore = concatenated_alignment.get_precision_recall_fscore(cipher.reference_alignment_data)\n",
    "\n",
    "    evaluation_details = {\n",
    "        \"csv_filename\": cipher.csv_filename,\n",
    "        'homophonicity': cipher.homophonicity,\n",
    "        'error': str(cipher.error.error_type if cipher.error is not None else None),\n",
    "        'error rate': str(cipher.error.error_rate if cipher.error is not None else None),\n",
    "        'length': len(cipher.plaintext),\n",
    "        \"checklength\": len(concatenated_alignment), \n",
    "        \"model\": model.NAME,\n",
    "        \"iterations\": model.n_iter,\n",
    "        \"segment_size\": cipher.seq_size,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": fscore,\n",
    "        #\"AER\": aer,\n",
    "        #\"Levenshtein Distance\": ld,\n",
    "        \"alignments\": concatenated_alignment if len(concatenated_alignment) < 1000 else 'Too many to display',\n",
    "    }\n",
    "    return evaluation_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block: DEFINE PARAMETERS  \n",
    "\n",
    "seq_sizes = [5]\n",
    "n_iters = [10]\n",
    "sample_size = 10 #100\n",
    "\n",
    "cipherkey_out_filename = generate_xlsx_file_name(\"cipherkey_evaluation\")\n",
    "alignment_out_filename = generate_xlsx_file_name(\"alignment_evaluation\")\n",
    "\n",
    "path_to_file = 'test_data100.csv'\n",
    "key_results = []\n",
    "alignment_results = []\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block:  LOAD AND PROCESS DATA. (Continued)\n",
    "for seq_size in seq_sizes:\n",
    "    sampled_lines = sample_lines_from_large_file(path_to_file, sample_size)\n",
    "    list_of_random_ciphers = []\n",
    "    \n",
    "    error_types = [\"addition\", \"deletion\", \"substitution\", \"duplication\", \"all\"]\n",
    "    for random_line in sampled_lines:\n",
    "        line = random_line.strip().split(';')\n",
    "        cipher_correct = Cipher(id_tag=line[0],plaintext=line[1],str_key=line[2],ciphertext=line[3],filename=line[4], homophonicity=line[5], csv_filename=path_to_file, seq_size=seq_size)\n",
    "        list_of_random_ciphers.append(cipher_correct)\n",
    "        for error_type in error_types:\n",
    "            error = ErrorSequence(line[3], error_type)\n",
    "            cipher_erroneous = Cipher(id_tag=line[0],plaintext=line[1],str_key=line[2],ciphertext=line[3],filename=line[4], homophonicity=line[5], csv_filename=path_to_file, error=error, seq_size=seq_size)\n",
    "            list_of_random_ciphers.append(cipher_erroneous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND EVALUATE MODELS\n",
    "\n",
    "for n_iter in n_iters:\n",
    "    for cipher in list_of_random_ciphers:\n",
    "        models = [NLTKIBMModelAdapter(cipher.bitext, n_iter, \"model2\", use_null=False)]\n",
    "        for model in models:\n",
    "            key_evaluation = train_and_evaluate_model(model, cipher)\n",
    "            key_results.append(KeyRecreationResult(cipher, [(model, key_evaluation)]))\n",
    "            alignment_evaluation = evaluate_alignments(model, cipher, continuous=True)\n",
    "            alignment_results.append(AlignmentResult(cipher, [alignment_evaluation]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(key_results, cipherkey_out_filename)\n",
    "write_alignment_to_file(alignment_results, alignment_out_filename)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Evaluation complete\")\n",
    "print(f\"Time taken: {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "The evaluation of the cipher alignment and key recreation models is complete. The results have been saved to the specified csv/Excel files. Below is a brief summary of the findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ck_filename=cipherkey_out_filename.replace('.xlsx', '.csv')\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_cipher = pd.read_csv(ck_filename)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_cipher.head()\n",
    "# Display the last few rows of the DataFrame\n",
    "df_cipher.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Plot precision, recall, and f1_score for different models\n",
    "df_cipher.plot(x='model', y=['precision', 'recall', 'f1_score'], kind='bar')\n",
    "plt.title('Model Evaluation Metrics')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Scores')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
